{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreServerAnn(object):\n",
    "    def __init__(self, text, comefrom = None):\n",
    "        self.text = text\n",
    "        self.source_sens = []\n",
    "        self.derived_sens = []\n",
    "        self.quests = []\n",
    "        self.comefrom = comefrom\n",
    "\n",
    "        \n",
    "class CoreServerQuest(object):\n",
    "    def __init__(self, text, comefrom):\n",
    "        self.text = text\n",
    "        self.comefrom = comefrom\n",
    "        \n",
    "\n",
    "class CoreServerSen(object):\n",
    "    def __init__(self, sen_dict, comefrom = None):\n",
    "        self.sen_dict = sen_dict\n",
    "        self.tokens = self.sen_dict['tokens']\n",
    "        self.words = self._get_words(self.tokens)\n",
    "        self.text = self._get_text(self.tokens)\n",
    "        self.comefrom = comefrom\n",
    "        self.entitymentions = self._get_entitymentions()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sen_dict['tokens'])\n",
    "    \n",
    "    def _get_text(self, tokens):\n",
    "        return \"\".join(token[\"word\"] for token in tokens)\n",
    "    \n",
    "    def _get_words(self, tokens):\n",
    "        return [token[\"word\"] for token in tokens]\n",
    "    \n",
    "    def _get_entitymentions(self):\n",
    "        return self.sen_dict.get('entitymentions', [])\n",
    "\n",
    "        \n",
    "class CoreServerAnalyzer(object):\n",
    "    def __init__(self, url = 'http://localhost:9000',\n",
    "                 properties = None,\n",
    "                 lang = \"zh\",\n",
    "                 timeout = 60\n",
    "                ):\n",
    "        self.url = url\n",
    "        self.timeout = timeout\n",
    "        self.lang = lang\n",
    "        self.timeout = timeout\n",
    "        if properties is None:\n",
    "            properties = {\"annotators\": \"tokenize,ssplit,pos,ner\",\n",
    "                          \"ssplit.boundaryTokenRegex\": \"[.。]|[!?！？]\",\n",
    "                          \"pipelineLanguage\": self.lang,\n",
    "                          \"'ner.applyFineGrained\": False\n",
    "                         }\n",
    "        self.properties = properties\n",
    "    \n",
    "    def __call__(self, ann, comefrom = None):\n",
    "        if comefrom is None:\n",
    "            comefrom = ann\n",
    "        resp = requests.post(self.url, data=ann.text.encode('utf8'), params = {\"properties\": str(self.properties)},\n",
    "                             timeout = self.timeout\n",
    "                            )\n",
    "        ann.source_sens = [CoreServerSen(sen_dict, comefrom) for sen_dict in resp.json()['sentences']]\n",
    "        return ann\n",
    "\n",
    "\n",
    "class CoreServerTranformer(object):\n",
    "    def __init__(self, max_len = None, by_punc = True, rule = None,\n",
    "                 url = \"http://localhost:9000/tregex\", lang = 'zh',\n",
    "                 timeout = 60,\n",
    "                 post_min_len = None,\n",
    "                 post_by_ner = False,\n",
    "                 entity_labels = None\n",
    "                ):\n",
    "        self.max_len = max_len\n",
    "        self.by_punc = by_punc\n",
    "        self.puncs = ['?!？！']\n",
    "        self.rule = rule\n",
    "        self.lang = lang\n",
    "        self.properties = {\"pipelineLanguage\": self.lang}\n",
    "        self.url = url\n",
    "        self.timeout = timeout\n",
    "        self.analyzer = CoreServerAnalyzer()\n",
    "        self.post_min_len = post_min_len\n",
    "        self.post_by_ner = post_by_ner\n",
    "        self.entity_labels = entity_labels\n",
    "        \n",
    "    def __call__(self, ann):\n",
    "        source_sens = ann.source_sens\n",
    "        kept_sens = self._filter(source_sens)\n",
    "        simplified_sens = self._simplify(kept_sens)\n",
    "        derived_sens = self._post_filter(simplified_sens)\n",
    "        ann.derived_sens = derived_sens\n",
    "        return ann\n",
    "        \n",
    "    def _filter(self, source_sens):\n",
    "        kept_sens = source_sens\n",
    "        if self.max_len is not None:\n",
    "            kept_sens = self._filter_by_len(kept_sens)\n",
    "        if self.by_punc:\n",
    "            kept_sens = self._filter_by_punc(kept_sens)\n",
    "        return kept_sens\n",
    "        \n",
    "    def _filter_by_len(self, sens):\n",
    "        kept_sens = [sen for sen in sens if len(sen) <= self.max_len]\n",
    "        return kept_sens\n",
    "    \n",
    "    def _filter_by_punc(self, sens):\n",
    "        kept_sens = [sen for sen in sens if sen.words[-1].strip() not in self.puncs]\n",
    "        return kept_sens\n",
    "    \n",
    "    def _simplify(self, kept_sens):\n",
    "        simplified_sens = []\n",
    "        for sen in kept_sens:\n",
    "            res = requests.post(self.url, data=sen.text.encode(\"utf8\"), params = {\"pattern\": self.rule, \"properties\": str(self.properties)},\n",
    "                                timeout = self.timeout\n",
    "                               )\n",
    "            res_dict = res.json()\n",
    "            sub_sens = res_dict['sentences'][0]\n",
    "            for sub_k, sub_v in sub_sens.items():\n",
    "                    sub_treestr = sub_v[\"match\"]\n",
    "                    sub_text = self._treestr2text(sub_treestr)\n",
    "                    tmp_ann = CoreServerAnn(sub_text)\n",
    "                    tmp_ann = self.analyzer(tmp_ann, sen)\n",
    "                    simplified_sens += tmp_ann.source_sens\n",
    "        return simplified_sens\n",
    "    \n",
    "    def _treestr2text(self, treestr):\n",
    "        pattern = r'[^\\(\\s\\)]+\\)'\n",
    "        words_ = re.findall(pattern, treestr)\n",
    "        words = [w_[:-1] for w_ in words_]\n",
    "        text = \"\".join(words)\n",
    "        return text\n",
    "    \n",
    "    def _post_filter(self, simplified_sens):\n",
    "        kept_sens = simplified_sens\n",
    "        if self.post_min_len is not None:\n",
    "            kept_sens = self._post_filter_by_len(kept_sens)\n",
    "        if self.post_by_ner:\n",
    "            kept_sens = self._post_filter_by_ner(kept_sens)\n",
    "        return kept_sens\n",
    "    \n",
    "    def _post_filter_by_len(self, sens):\n",
    "        kept_sens = [sen for sen in sens if len(sen) >= self.post_min_len]\n",
    "        return kept_sens\n",
    "    \n",
    "    def _post_filter_by_ner(self, sens):\n",
    "        def check_entity(sen):\n",
    "            entitymentions = sen.entitymentions\n",
    "            if entitymentions == []:\n",
    "                return False\n",
    "            \n",
    "            if any(em['ner'] in self.entity_labels for em in entitymentions) == False:\n",
    "                return False\n",
    "            \n",
    "            return True\n",
    "        kept_sens = [sen for sen in sens if check_entity(sen)]\n",
    "        return kept_sens\n",
    "    \n",
    "\n",
    "class CoreServerTransducer(object):\n",
    "    def __init__(self, entity2quest):\n",
    "        self.entity2quest = entity2quest\n",
    "\n",
    "    \n",
    "    def __call__(self, ann):\n",
    "        derived_sens = ann.derived_sens\n",
    "        quests = self._gen_quests(derived_sens)\n",
    "        ann.quests = quests\n",
    "        return ann\n",
    "   \n",
    "    def _gen_quests(self, sens):\n",
    "        quests = []\n",
    "        for sen in sens:\n",
    "            qs = self._sen2quests(sen)\n",
    "            quests += qs\n",
    "        return quests\n",
    "    \n",
    "    def _sen2quests(self, sen):\n",
    "        entitymentions = sen.entitymentions\n",
    "        sen_text = sen.text\n",
    "        quests = []\n",
    "        for em in entitymentions:\n",
    "            ner = em['ner']\n",
    "            cb, ce = em['characterOffsetBegin'], em['characterOffsetEnd']\n",
    "            if ner in self.entity2quest:\n",
    "                quest = sen_text[:cb] + self.entity2quest[ner] + sen_text[ce:]\n",
    "                quests.append(CoreServerQuest(quest, sen))\n",
    "        return quests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"公民申请普通护照，应当由本人向其户籍所在地县级以上地方人民政府公安机关出入境管理机构提出，并提交以上真实有效的材料，现役军人按照管理权限履行报批手续后，由本人向所属部队驻地县级以上地方人民政府公安机关出入境管理机构提出。\"\n",
    "# text =  '何洛洛在群访中回应称自己一定是会去高考的，虽然今年错过了，\\\n",
    "# 但明年一定会全力以赴。何洛洛坦言每个人在追梦的道路上都有自己的选择和机会，\\\n",
    "# 他会对自己的选择全力以赴坚持到底。至于错过今年高考是否遗憾，\\\n",
    "# 何洛洛给出了否定的答案，“每个人都有自己的选择，既然选择了《创造营2019》，那我明年继续备战高考。”'\n",
    "\n",
    "# text = \"作为山东人具体一年到头要吃几顿饺子真的没有具体数字，想吃了就调上馅儿包上咱就吃说说我们这儿必须吃饺子的日子：小年吃、大年三十12点钟声一敲必须吃饺子、正月初二送年的时候吃、过冬至的时候吃（俗语是为了防止冻耳朵），有人要远行一定要吃送行的饺子（俗语上车的饺子下车的面）还有嫁女儿一定要吃饺子，俗语叫做“滚蛋饺”（哈哈）\"\n",
    "text = \"赵文华先生是德勤咨询公司一位出色的合伙人，他于十几年前来到了中国。由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\"\n",
    "analyzer = CoreServerAnalyzer()\n",
    "ann = CoreServerAnn(text)\n",
    "ann = analyzer(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.CoreServerSen at 0x26925ebce10>,\n",
       " <__main__.CoreServerSen at 0x26925ebce48>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.source_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LOCATION', 'PERSON', 'GPE', 'ORGANIZATION']\n"
     ]
    }
   ],
   "source": [
    "rule = \"IP<(NP=np $..(VP=vp ?$.. PU)) >(ROOT|IP|CP)\"\n",
    "# rule = \"ROOT < (IP<(NP=np $.. VP=vp))\"\n",
    "entity2quest = {\"LOCATION\": \"{ 哪里 | 什么地方 }\",\n",
    "                \"PERSON\": \"{ 谁 }\",\n",
    "                \"GPE\": \"{ 哪里 | 什么地方 }\",\n",
    "                \"ORGANIZATION\": \"{ 什么 组织|机构 }\"\n",
    "               }\n",
    "entity_labels = list(entity2quest.keys())\n",
    "print(entity_labels)\n",
    "tranformer = CoreServerTranformer(rule = rule, entity_labels = entity_labels,\n",
    "                                  post_by_ner= True\n",
    "                                 )\n",
    "ann = tranformer(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.CoreServerSen at 0x26925f320b8>,\n",
       " <__main__.CoreServerSen at 0x26925f46390>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.derived_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "赵文华先生是德勤咨询公司一位出色的合伙人\n",
      "由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n"
     ]
    }
   ],
   "source": [
    "for tmp_sen in ann.derived_sens:\n",
    "    print(tmp_sen.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'docTokenBegin': 0, 'docTokenEnd': 1, 'tokenBegin': 0, 'tokenEnd': 1, 'text': '赵文华', 'characterOffsetBegin': 0, 'characterOffsetEnd': 3, 'ner': 'PERSON'}, {'docTokenBegin': 3, 'docTokenEnd': 6, 'tokenBegin': 3, 'tokenEnd': 6, 'text': '德勤咨询公司', 'characterOffsetBegin': 6, 'characterOffsetEnd': 12, 'ner': 'ORGANIZATION'}, {'docTokenBegin': 6, 'docTokenEnd': 7, 'tokenBegin': 6, 'tokenEnd': 7, 'text': '一', 'characterOffsetBegin': 12, 'characterOffsetEnd': 13, 'ner': 'NUMBER', 'normalizedNER': '1'}]\n",
      "[{'docTokenBegin': 1, 'docTokenEnd': 2, 'tokenBegin': 1, 'tokenEnd': 2, 'text': '赵文华', 'characterOffsetBegin': 1, 'characterOffsetEnd': 4, 'ner': 'PERSON'}, {'docTokenBegin': 5, 'docTokenEnd': 8, 'tokenBegin': 5, 'tokenEnd': 8, 'text': '德勤分析研究院', 'characterOffsetBegin': 9, 'characterOffsetEnd': 16, 'ner': 'ORGANIZATION'}]\n"
     ]
    }
   ],
   "source": [
    "for tmp_sen in ann.derived_sens:\n",
    "    print(tmp_sen.entitymentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transducer = CoreServerTransducer(entity2quest)\n",
    "ann = transducer(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.CoreServerQuest at 0x26925ebc3c8>,\n",
       " <__main__.CoreServerQuest at 0x26925ebcef0>,\n",
       " <__main__.CoreServerQuest at 0x26925ebc358>,\n",
       " <__main__.CoreServerQuest at 0x26925ebc2b0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.quests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "问： { 谁 }先生是德勤咨询公司一位出色的合伙人\n",
      "答1： 赵文华先生是德勤咨询公司一位出色的合伙人\n",
      "答2： 赵文华先生是德勤咨询公司一位出色的合伙人\n",
      "答3： 赵文华先生是德勤咨询公司一位出色的合伙人，他于十几年前来到了中国。\n",
      "\n",
      "\n",
      "问： 赵文华先生是{ 什么 组织|机构 }一位出色的合伙人\n",
      "答1： 赵文华先生是德勤咨询公司一位出色的合伙人\n",
      "答2： 赵文华先生是德勤咨询公司一位出色的合伙人\n",
      "答3： 赵文华先生是德勤咨询公司一位出色的合伙人，他于十几年前来到了中国。\n",
      "\n",
      "\n",
      "问： 由{ 谁 }先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "答1： 由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "答2： 由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "答3： 由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "\n",
      "\n",
      "问： 由赵文华先生领导的{ 什么 组织|机构 }为多家合作伙伴提供了卓越的解决方案。\n",
      "答1： 由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "答2： 由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "答3： 由赵文华先生领导的德勤分析研究院为多家合作伙伴提供了卓越的解决方案。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in ann.quests:\n",
    "    print(\"问：\", q.text)\n",
    "    print(\"答1：\", q.comefrom.text)\n",
    "    print(\"答2：\", q.comefrom.text)\n",
    "    print(\"答3：\", q.comefrom.comefrom.text)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
